{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You''ll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures (1): Vocab, Lexemes and StringStore\n",
    "\n",
    "Welcome back! Now that you've had some real experience using spaCy's objects, it's time for you to learn more about what's actually going on under spaCy's hood.\n",
    "\n",
    "In this lesson, we'll take a look at the shared vocabulary and how spaCy deals with strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared vocab and string store (1)\n",
    "\n",
    "- `Vocab`: stores data shared across multiple documents\n",
    "- To save memory, spaCy encodes all strings to hash values\n",
    "- Strings are only stored once in the StringStore via nlp.vocab.strings\n",
    "- String store: lookup table in both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1198eb8174f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The second line will raise an exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcoffee_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coffee\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcoffee_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoffee_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "# The second line will raise an exception\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3197928453018144401, 'coffee')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strangely once you process a document with this word, the exception disappears\n",
    "nlp(\"This coffee is really good\")\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "coffee_hash, coffee_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy stores all shared data in a vocabulary, the Vocab.\n",
    "\n",
    "This includes words, but also the labels schemes for tags and entities.\n",
    "\n",
    "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
    "\n",
    "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp.vocab.strings.\n",
    "\n",
    "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
    "\n",
    "Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149, 1149)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I love yxxxxcoffee\")\n",
    "len(doc.vocab.strings), len(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To get the hash for a string, we can look it up in nlp.vocab.strings.\n",
    "\n",
    "To get the string representation of a hash, we can look up the hash.\n",
    "\n",
    "A `Doc` object also exposes its vocab and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.vocab.Vocab"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lexeme.Lexeme"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.vocab[\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.strings.StringStore"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197928453018144401"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab.strings[\"coffee\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexemes: entiries in the vocabulary\n",
    "\n",
    "A Lexeme object is an entry in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('coffee', 3197928453018144401, True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print lexical attributes\n",
    "lexeme.text, lexeme.orth, lexeme.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contains the context-independent information about a word\n",
    "    - Word text: `lexeme.text` and `lexeme.orth` (the hash)\n",
    "    - Lexical attributes like `lexeme.is_alpha`\n",
    "    - Not context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexemes are context-independent entries in the vocabulary.\n",
    "\n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
    "\n",
    "Lexemes expose attributes, just like tokens.\n",
    "\n",
    "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.\n",
    "\n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab, hashes and lexemes\n",
    "![](https://course.spacy.io/vocab_stringstore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know all about the vocabulary and string store, we can take a look at the most important data structure: the `Doc`, and its views `Token` and `Span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the `nlp` object. But you can also instantiate the class manually.\n",
    "\n",
    "After creating the nlp object, we can import the `Doc` class from `spacy.tokens`.\n",
    "\n",
    "Here we're creating a doc from three words. The spaces are a list of boolean values indicating whether the word is followed by a space. Every token includes that information – even the last one!\n",
    "\n",
    "The Doc class takes three arguments: the shared vocab, the words and the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp(\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Span object(1)\n",
    "\n",
    "![](https://course.spacy.io/span_indices.png)\n",
    "\n",
    "A `Span` is a slice of a doc consisting of one or more tokens. The `Span` takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Hello world,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Span` manually, we can also import the class from `spacy.tokens`. We can then instantiate it with the doc and the span's start and end index, and an optional label argument.\n",
    "\n",
    "The `doc.ents` are writable, so we can add entities manually by overwriting it with a list of spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "- Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "    - Convert result to strings as late as possible\n",
    "    - Use token attributes if available – for example, token.i for the token index\n",
    "- Don't forget to pass in the shared vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few tips and tricks before we get started:\n",
    "\n",
    "The `Doc` and `Span` are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "\n",
    "If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
    "\n",
    "To keep things consistent, try to use built-in token attributes wherever possible. For example, `token.i` for the token index.\n",
    "\n",
    "Also, don't forget to always pass in the shared vocab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and semantic similarities\n",
    "\n",
    "In this lesson, you'll learn how to use spaCy to predict how similar documents, spans or tokens are to each other.\n",
    "\n",
    "You'll also learn about how to use word vectors and how to take advantage of them in your NLP application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing semantic similarity\n",
    "- spaCy can compare two objects and predict similarity\n",
    "- `Doc.similarity()`, `Span.similarity()` and `Token.similarity()`\n",
    "- Take another object and return a similarity score (0 to 1)\n",
    "- Important: needs a model that has word vectors included, for example:\n",
    "    - ✅ en_core_web_md (medium model)\n",
    "    - ✅ en_core_web_lg (large model)\n",
    "    - 🚫 NOT en_core_web_sm (small model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
